---
title: Examining spatial stratification
execute:
    cache: true
output:
    html_document:
        toc: true
        toc_float: true
        code_folding: hide
        number_sections: true
        theme: readable
        keep_md: true
---






## Background

To what degree does simple spatial confounding get captured / adjusted by PCs?

1. Simulate a confounder which is a spatial gradient from a random centroid in UK Biobank samples
2. Simulate a phenotype which is a function of the confounder and some noise
3. Simulate a variant where the allele frequency is a function of the confounder
4. Examine the association between the phenotype and the variant, with and without PCs and other adjustments

The overall objective is to avoid confounding by capturing the spatial structure of the phenotype.

```
G <--- demography -x-> P
```


## Data

```bash
# #!/bin/bash

datadir="/mnt/storage/private/mrcieu/data/ukbiobank/phenotypic/applications/81499/released/2022-06-07/data"

head -n 1 $datadir/data.51913.csv | tr ',' '\n' > ukb.datafields

less ukb.datafields 

grep -n "^\"129-" ukb.datafields | cut -d ":" -f 1 
grep -n "^\"130-" ukb.datafields | cut -d ":" -f 1 


cut -d "," -f 1,435-440 $datadir/data.51913.csv > ukb.coords
cut -d "," -f 10005-10044 $datadir/data.51913.csv > ukb.pcs
```

Note this way of extracting data is not reliable because of the assumption of comma dilimeted fields. Link the dataset PCs to the phenotype ID

```{r}
library(dplyr)
library(data.table)
library(ggplot2)
library(here)
library(nnet)
library(caret)
library(parallel)
library(hexbin)

coords <- fread(here("ukb.coords")) %>% filter(!is.na(`130-0.0`))
names(coords)[2] <- "northing"
names(coords)[5] <- "easting"

# pcs <- fread("/mnt/storage/private/mrcieu/data/ukbiobank/genetic/variants/arrays/imputed/released/2018-09-18/data/derived/principal_components/data.pca1-40.plink.txt")
pcs <- fread(here("data.pca1-40.plink.txt"))

# linker <- fread("/mnt/storage/private/mrcieu/data/ukbiobank/phenotypic/applications/81499/released/2022-06-07/data/linker.81499.csv")
linker <- fread(here("linker.81499.csv"))

pcs <- inner_join(pcs, linker, by=c("V1"="ieu"))
dat <- inner_join(pcs, coords, by=c("app"="eid"))

```

```{r}
str(dat)
```

```{r}
str(pcs)
```

## Simulations



```{r}
fit_genetic_nn <- function(y, genetic_pcs, 
                          hidden_units = 10, 
                          decay = 0.001,
                          train_prop = 0.8,
                          scale_data = TRUE,
                          set_seed = 123, maxit=100) {
  
  # Set seed for reproducibility
  set.seed(set_seed)
  
  # Combine data
  data <- data.frame(y = y, genetic_pcs)
  
  # Remove any rows with missing values
  data <- na.omit(data)
  n_samples <- nrow(data)
  
#   cat("Sample size after removing NAs:", n_samples, "\n")
#   cat("Number of genetic PCs:", ncol(genetic_pcs), "\n")
  
  # Split into training and testing sets
  train_indices <- sample(1:n_samples, size = floor(train_prop * n_samples))
  train_data <- data[train_indices, ]
  test_data <- data[-train_indices, ]
  
  # Scale the predictors if requested
  if (scale_data) {
    # Calculate scaling parameters from training data
    pc_means <- apply(train_data[, -1], 2, mean)
    pc_sds <- apply(train_data[, -1], 2, sd)
    
    # Scale training data
    train_data[, -1] <- scale(train_data[, -1])
    
    # Scale test data using training parameters
    test_data[, -1] <- scale(test_data[, -1], center = pc_means, scale = pc_sds)
  }
  
  # Fit neural network
#   cat("Fitting neural network with", hidden_units, "hidden units...\n")
  
  nn_model <- nnet(y ~ ., 
                   data = train_data,
                   size = hidden_units,
                   decay = decay,
                   linout = TRUE,  # Linear output for regression
                   trace = FALSE,  # Suppress training output
                   maxit = maxit)   # Maximum iterations
  
  # Make predictions on all data
  if (scale_data) {
    # Scale full dataset for predictions
    full_data_scaled <- data
    full_data_scaled[, -1] <- scale(data[, -1], center = pc_means, scale = pc_sds)
    predictions <- predict(nn_model, full_data_scaled[, -1])
  } else {
    predictions <- predict(nn_model, data[, -1])
  }
  
  # Calculate residuals
  residuals <- data$y - predictions
  
  # Calculate performance metrics
  train_predictions <- predict(nn_model, train_data[, -1])
  test_predictions <- predict(nn_model, test_data[, -1])
  
  train_rmse <- sqrt(mean((train_data$y - train_predictions)^2))
  test_rmse <- sqrt(mean((test_data$y - test_predictions)^2))
  train_r2 <- cor(train_data$y, train_predictions)^2
  test_r2 <- cor(test_data$y, test_predictions)^2
  
  # Print performance
#   cat("\nModel Performance:\n")
#   cat("Training RMSE:", round(train_rmse, 4), "\n")
#   cat("Test RMSE:", round(test_rmse, 4), "\n")
#   cat("Training R²:", round(train_r2, 4), "\n")
#   cat("Test R²:", round(test_r2, 4), "\n")
  
  # Return results
  results <- list(
    model = nn_model,
    predictions = predictions,
    residuals = residuals,
    train_indices = train_indices,
    performance = list(
      train_rmse = train_rmse,
      test_rmse = test_rmse,
      train_r2 = train_r2,
      test_r2 = test_r2
    ),
    scaling_params = if (scale_data) list(means = pc_means, sds = pc_sds) else NULL
  )
  
  return(results)
}

generate_conf <- function(coord1, coord2, centroid, sharpness) {
    # Calculate the distance from the centroid
    distance_from_centroid <- sqrt((coord1 - centroid[1])^2 + (coord2 - centroid[2])^2)
    # Frequency of the variant reduces with distance according to sharpness
    frequency <- distance_from_centroid - min(distance_from_centroid, na.rm=T)
    frequency <- frequency^sharpness
    frequency <- 1 - frequency / max(frequency, na.rm=T)    
}

generate_variant_based_on_location <- function(conf)  {
    variant <- rbinom(length(conf), size = 2, prob = conf)
    return(variant)
}

generate_phen <- function(conf, variant, b_conf, b_variant) {
    phen <- scale(conf) * b_conf + scale(variant) * b_variant + rnorm(length(conf), 0, sqrt(1 - b_conf^2 - b_variant^2))
    return(drop(phen))
}

dgm <- function(centroid, sharpness, b_conf, b_variant, coord1=dat$northing, coord2=dat$easting) {
    conf <- generate_conf(coord1, coord2, centroid, sharpness)
    variant <- generate_variant_based_on_location(conf)
    phen <- generate_phen(conf, variant, b_conf, b_variant)
    return(tibble(variant=variant, conf=conf, phen=phen, northing=coord1, easting=coord2))
}

plot_uk_density <- function(phendat) {
    p1 <- ggplot(phendat, aes(x=easting, y=northing)) +
        stat_summary_hex(aes(z=phen), fun = mean) +
        labs(title="Spatial distribution of phenotypes", x="Easting", y="Northing") +
        theme_minimal() +
        theme(legend.position = "bottom")
    p2 <- ggplot(phendat, aes(x=easting, y=northing)) +
        stat_summary_hex(aes(z=variant), fun = mean) +
        labs(title="Spatial distribution of variants", x="Easting", y="Northing") +
        theme_minimal() +
        theme(legend.position = "bottom")
    p3 <- ggplot(phendat, aes(x=easting, y=northing)) +
        stat_summary_hex(aes(z=conf), fun = mean) +
        labs(title="Spatial distribution of confounders", x="Easting", y="Northing") +
        theme_minimal() +
        theme(legend.position = "bottom")
    gridExtra::grid.arrange(p1, p2, p3, ncol=3)
}

mreg <- function(y, x) {
    x <- model.matrix(~ x)
    beta <- solve(t(x) %*% x) %*% t(x) %*% y
    pred <- x %*% beta
    residuals <- y - pred
    return(residuals)
}

hexbins_adjustment <- function(phendat) {
    bins <- hexbin(phendat$easting, phendat$northing, 
                   xbins = 30, 
                   IDs = TRUE)
    phendat$hex_id <- bins@cID
    # lm(phen ~ as.factor(hex_id), data=phendat)$residuals -> phendat$phen_resid
    phendat$phen_resid <- mreg(phendat$phen, phendat$hex_id)
    summary(lm(phen_resid ~ variant, data=phendat))
}

estimation <- function(dat, phendat) {
    mod1 <- summary(lm(phen ~ scale(variant) + conf, data=phendat))
    mod2 <- summary(lm(phen ~ scale(variant), data=phendat))
    mod3 <- summary(lm(phen ~ scale(variant) + as.matrix(dat[,3:42]), data=phendat))
    mod4 <- summary(lm(phen ~ scale(variant) + northing + easting, data=phendat))
    coord_pred <- fit_genetic_nn(
        y = phendat$phen,
        genetic_pcs = phendat %>% select(northing, easting),
        hidden_units = 10,
        decay = 0.001,
        scale_data = TRUE,
        maxit=25
    )$predictions
    mod5 <- summary(lm(phen ~ scale(variant) + coord_pred, data=phendat))
    mod6 <- hexbins_adjustment(phendat)
    tibble(
        model = c("variant + conf", "variant", "variant + PCs", "variant + coords", "variant + coord_nn", "variant + hexbins"),
        freq = mean(phendat$variant)/2,
        var_beta = c(mod1$coefficients[2,1], mod2$coefficients[2,1], mod3$coefficients[2,1], mod4$coefficients[2,1], mod5$coefficients[2,1], mod6$coefficients[2,1]),
        var_se = c(mod1$coefficients[2,2], mod2$coefficients[2,2], mod3$coefficients[2,2], mod4$coefficients[2,2], mod5$coefficients[2,2], mod6$coefficients[2,2]),
        var_p = c(mod1$coefficients[2,4], mod2$coefficients[2,4], mod3$coefficients[2,4], mod4$coefficients[2,4], mod5$coefficients[2,4], mod6$coefficients[2,4]),
        r2 = c(mod1$r.squared, mod2$r.squared, mod3$r.squared, mod4$r.squared, mod5$r.squared, mod6$r.squared),
        adj_r2 = c(mod1$adj.r.squared, mod2$adj.r.squared, mod3$adj.r.squared, mod4$adj.r.squared, mod5$adj.r.squared, mod6$adj.r.squared),
        n = nrow(phendat)
    )
}

run_sim <- function(centroid, sharpness, b_conf, b_variant, rep=1) {
    args <- tibble(centroid=paste(centroid, collapse=","), sharpness=sharpness, b_conf=b_conf, b_variant=b_variant, rep=rep)
    phendat <- dgm(centroid, sharpness, b_conf, b_variant)
    est <- estimation(dat, phendat)
    bind_cols(args, est)
}
```

Example simulation

- sharpness is 1 (i.e. completely smooth)
- common variant
- strong confounding
- null variant

```{r}
set.seed(111)
i <- sample(1:nrow(dat), 1)
phendat <- dgm(c(dat$northing[i], dat$easting[i]), sharpness=1, b_conf = 0.05, b_variant = 0)
plot_uk_density(phendat)
```

Estimation

Methods:

- `variant + conf`: captures the actual spatial variable
- `variant`: No adjustments
- `variant + PCs`: Adjusts for 40 common variant PCs
- `variant + coords`: Adjusts for northing and easting
- `variant + coord_nn`: Adjusts for a neural network model of the coordinates, trained to fit the phenotype
- `variant + hexbins`: Adjusts for spatial stratification using fixed effects adjustment for 404 hexagonal bins (i.e. within area effect estimates)


```{r}
estimation(dat, phendat)
```

PCs do quite a bad job of capturing any confounding. Using non-linear coordinates works relatively well, but linear coordinates and hexbins do not work well.

Try again with much sharper spatial stratification

```{r}
set.seed(111)
i <- sample(1:nrow(dat), 1)
phendat_sharp <- dgm(c(dat$northing[i], dat$easting[i]), sharpness=0.001, b_conf = 0.05, b_variant = 0)
plot_uk_density(phendat_sharp)
```

```{r}
estimation(dat, phendat_sharp)
```

Now everything works badly.


Try non-linear PCs

```{r}

```


```{r, eval=FALSE}
# Choose 100 random samples from the UK Biobank data to use as centroids
centroids <- dat %>%
    filter(!is.na(northing) & northing > 0) %>%
    select(northing, easting) %>%
    slice_sample(n=100)
str(centroids)

param <- expand.grid(
    centroid = 1:10,
    sharpness = c(0.001, 0.01, 0.1, 0.5),
    b_conf = c(0, 0.3),
    b_variant = c(0, 0.1),
    rep= 1:10
)
dim(param)

mclapply(1:nrow(param), function(i) {
    message(paste("Running row", i, "of", nrow(param)))
    tryCatch(
        run_sim(
            centroid = c(centroids$northing[param$centroid[i]], centroids$easting[param$centroid[i]]),
            sharpness = param$sharpness[i],
            b_conf = param$b_conf[i],
            b_variant = param$b_variant[i],
            rep = param$rep[i]
        ),
        error = function(e) {
            message(paste("Error in row", i, ":", e$message))
            return(NULL)
        }
    )   
}, mc.cores=60) %>% bind_rows() -> res
saveRDS(res, here("results", "stratification_sim_1.rds"))
```


```{r}
res <- readRDS(here("results", "stratification_sim_1.rds"))
ggplot(res, aes(x=freq, y=var_beta - b_variant, colour=model)) +
    geom_point() +
    labs(x="Frequency of variant", y="Estimated beta - true beta") +
    scale_colour_brewer(type = "qual") +
    facet_grid(b_conf ~ b_variant, labeller = label_both) +
    theme_minimal()
```

```{r}
p1 <- res %>%
ggplot(., aes(x=as.factor(sharpness), y=var_beta - b_variant, colour=model)) +
    geom_boxplot() +
    labs(x="Frequency of variant", y="Estimated beta - true beta") +
    scale_colour_brewer(type = "qual") +
    facet_grid(b_conf ~ b_variant, labeller = label_both) +
    theme_minimal()
p1
# ggsave(p1, file="temp.pdf", width=14, height=7)
```
